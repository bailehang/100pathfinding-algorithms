
# Repairing A\*、ARA\*、LPA\*、LRTA\*、RTAA\* 算法对比分析

## 时间线排序

按照提出时间先后，这些 A\* 算法变体的发表时间线如下：

* **1988/1990 – Learning Real-Time A**\*（**LRTA**\*）：由 **Korf** 提出，是最早的实时启发式搜索算法之一。
* **1994/1995 – Repairing A**\*（动态A\*，如 **D**\*）：由 **Stentz** 提出动态 A\* 算法，用于增量修复路径以应对环境变化。
* **2001/2004 – Lifelong Planning A**\*（**LPA**\*）：由 **Koenig 和 Likhachev** 提出，2004年在 AI Journal 发表，2001年已首次描述了该算法。
* **2003 – Anytime Repairing A**\*（**ARA**\*）：由 **Likhachev、Gordon、Thrun** 提出，在 2003 年作为高效任意时间启发式搜索算法报道。
* **2006 – Real-Time Adaptive A**\*（**RTAA**\*）：由 **Koenig 等** 在 2006 年提出，于 AAMAS 2006 大会上发表。

下面将对每种算法的特点、优缺点及应用场景进行分析，并附上对比表格总结它们的异同。

## 各算法的优缺点与应用

### Repairing A\*（动态/增量A\*）

**基本思路**：**Repairing A**\*（本文以动态 A\* 为代表）是一类能够在**环境变化时修复已有路径**的算法。典型代表是 **D**\*（Dynamic A\*）算法，由 Stentz 于1994–1995年提出。D\* 通过增量地更新先前的搜索信息，当图的边权发生变化（例如机器人发现新的障碍）时，无需从头运行 A\*，而是**修复受影响的路径**并找到新的最优解。它最初用于**机器人导航**中，在地图逐步探索或发生变化时实时调整路径，在 DARPA 无人地面车辆计划、NASA 火星车等项目中获得应用。

**优点**：Repairing A\* 算法的最大优点是**适应动态环境**的能力。当路径规划问题与之前相似时，它能够重用之前的计算结果，避免每次重新规划的开销。例如，动态 A\* 可在机器人移动过程中高效地对路径进行调整，使得机器人不必停下来重新规划全部路径。这类增量方法在环境变化**幅度较小时效率尤其高**，能显著减少重新规划的计算量。

**缺点**：其不足在于实现和维护的复杂性。早期的 D\* 算法需要对已搜索的节点进行较复杂的状态管理（这促使后来出现了更简单的变体如 D\* Lite）。另外，**增量修复的效率取决于环境变化的程度**：如果每次变化幅度很大（例如路径代价整体剧烈改变），算法可能需要遍历大部分节点，最坏情况下性能接近重新从头规划一次。因此，当环境变化频繁且剧烈时，增量修复的优势会降低。另外，初始规划仍需要一次完整的 A\* 搜索，这对大型图而言本身可能耗时。

**应用场景**：Repairing A\* 广泛用于**机器人导航和动态环境地图规划**。例如，移动机器人在未知环境中探索前进时，动态A*可随着机器人传感器发现新障碍不断调整路线。在游戏AI中，如果地图会在角色移动过程中发生变化（如即时战略游戏中地图被逐步揭露），此类算法也可用于在**动态更新**的地图上快速 replanning。总的来说，凡是**路径环境可能变化**的场景（未知地图探索、动态障碍物出现等），增量式的 Repairing A* 方法都非常适用。

### ARA\*（Anytime Repairing A\*）

**基本思路**：**ARA**\* 是一种**任意时间启发式搜索算法**，由 Likhachev 等人在 2003 年提出。它的核心思想是在有限计算时间内**尽快找到一条可行路径**，然后逐步改进该路径的质量。ARA\* 具体做法是：首先使用一个**放大的启发式权重** (w > 1) 运行 A\*，快速得到一条次优解路径；随后逐步减小启发式权重（例如 w 从较大值逐渐降至1），重复调用 A\* 搜索**但不丢弃前一次搜索的信息**，从而“修复”先前解并不断逼近最优。随着时间推进，ARA\* 会 reuse 已扩展过的节点和计算结果，不重复无效工作，使每次迭代的效率更高，**在有限时间内尽可能优化路径代价**。如果给予充足时间，ARA\* 最终能找到最优解，并在整个过程中提供解的次优性界限保证。

**优点**：ARA\* 将**快速响应**和**高解质量**结合起来。首次搜索用较松的启发得到的解非常迅速，而之后的迭代改进又确保可以逐渐提升路径质量。与简单地多次运行加权A*相比，ARA* **重用以前搜索过程中计算的启发信息和部分搜索树**，因此效率显著更高。文献报道，在六自由度机械臂路径规划、移动机器人路径规划等任务中，ARA\* 比每次从头规划的耗时降低了**约6倍**。此外，ARA\* 始终维持一个明确的次优解上界，使用户可以随时中断算法并知道当前解距最优的差距。

**缺点**：ARA\* 的主要限制是**其假定环境静态不变**。它针对的是在**固定地图/问题**下优化解的过程，如果在规划过程中环境（图）发生变化，ARA\* 并不会自行适应（需要结合增量算法，如将 ARA\* 与 D\* Lite 结合形成 Anytime Dynamic A\*）。另外，ARA\* 需要合理设置初始启发权重w的值：w过大会得到质量很差的初始路径，w过小则初解耗时变长。因此实际应用中往往需要折中选择启发权重策略。实现方面，ARA\* 相比标准 A\* 更复杂，需要维护在多次迭代中节点的开启/关闭列表状态以重用搜索树，这增加了实现难度。但这些开销相对于大幅提升的效率是值得的。

**应用场景**：ARA\* 适用于**需要即时获得可行方案且有望进一步优化解的场景**。例如机器人在复杂环境中规划路径但时间有限，可以先用 ARA\* 得到一条次优路径开始执行，同时后台继续优化路径；在自动驾驶、机械臂运动规划等需要平衡规划时间和路径质量的场合，ARA*可以提供分阶段优化的方案。在游戏AI中，如果要求 NPC 在极短思考时间内先行动起来，然后逐渐优化行动路径（尽管游戏中通常路径规划环境不变，但时间紧迫），ARA* 也具备优势。总之，在**求解质量和计算时间存在权衡**的问题中（启发式搜索或路径规划），ARA\* 是一种有效方法。

### LPA\*（Lifelong Planning A\*）

**基本思路**：**LPA**\* 算法由 Koenig 和 Likhachev 等人在 2001 年提出（2004 年正式发表）。它是 A\* 的**增量版本**，旨在**重复求解系列相关路径规划问题**时提高效率。LPA\* 假设起点和终点固定，但图的边权可能变化，或者有节点/边动态加入移除。算法在第一次规划时表现与 A\* 相同，找到初始最短路径；当图发生小变化需要重新规划时，LPA\* **重用之前搜索树中与当前问题相同的部分**，只对受变化影响的节点进行更新。具体来说，LPA\* 维护每个节点的 g 值和 rhs 值，通过一个类似于 A\* 的优先队列（OPEN表）来逐步修正受影响区域的路径成本，调用 `UpdateVertex` 等过程增量更新最短路径树。因此，当每次变化较小时，LPA\* 常能比从零开始运行 A\* 更快地找到新的最短路径。

**优点**：LPA\* **针对逐步变化的规划问题优化**。当每次图的改动很小且局部时（尤其是变化发生在靠近目标的位置），LPA\* 可以大幅减少重新规划的计算量。它很好地继承了 A\* 的启发式最优性保证，每次 replanning 仍然找到全局最优路径。同时，由于只更新必要部分，能**节省大量重复计算**。LPA\* 提供了一套统一框架，许多后续增量规划算法（如 D\* Lite）都建立在 LPA*思想之上，但实现更简洁。总的来说，LPA* 在需要\*\*多次规划（lifelong planning）\**的情境下比反复调用 A* 更高效。

**缺点**：LPA\* 的效率优势取决于**相邻两次规划问题的相似度**。如果每次变化范围很大（例如起点变化或大面积障碍变化），LPA\* 可能需要更新几乎整个搜索树，此时性能退化接近普通 A\*。因此在**剧烈变化**的动态环境中，其优势不如在小改动情形明显。此外，LPA\* 需要在内存中维护先前搜索的信息（OPEN表、g值等），对于超大规模图会有额外的内存开销。相比原始 A\*，LPA\* 的实现稍复杂，需要确保数据结构的一致性（例如每次变化后正确维护OPEN表）。但这些开销通常是可控的，也是换取增量性能提升所必须的。

**应用场景**：LPA\* 适用于**需要反复规划路径的长期运行任务**。经典应用是在**机器人或移动智能体反复规划**的场景中：例如机器人在环境中持续运动，一边行进一边根据新信息更新路径，LPA\* 可以快速调整路径（D\* Lite 就是利用 LPA\* 原理用于机器人导航）。在动态网络路由、动态地图导航等需要频繁重规划的领域，LPA\* 同样有用。在游戏AI中，如果地图环境会逐渐变化或者角色需要多次往返于不同目标之间，也可运用LPA*减少每次路径计算时间。总之，**当规划问题会随着时间推移发生小幅修改且需要多次求解**时，LPA*能显著提高效率。

### LRTA\*（Learning Real-Time A\*）

**基本思路**：**LRTA**\* 算法是由 Korf 于1990年前后提出的**实时启发式搜索**方法。与传统的离线A*一次性规划完整路径不同，LRTA* 采取**边搜索边执行**的在线策略：智能体每次只向前规划有限深度（通常为一步）并执行移动，然后根据实际走过的路径对启发值进行学习更新，接着从新状态再规划下一步，如此迭代。通过这种方式，LRTA\* 在每个行动决策点只消耗很短的规划时间，确保了**实时性**。其名称中的“Learning”意味着算法在搜索过程中**逐渐更新启发函数**使其更加准确，从而在反复执行类似任务时性能改善。Korf 证明了，在静态环境多次往返后，LRTA\* 的启发值会收敛，最终能够学得最优解的启发估计，使得路径趋近最优。

**优点**：LRTA\* 的最大优势是能满足**严格的实时响应要求**。每当智能体需要采取动作时，LRTA\* 都能在限定的极短时间内给出一个行动方向，不会因为长时间规划而卡顿。这在需要连续平滑移动的场景（例如即时战略游戏中的单位移动、即时响应的机器人）中非常重要。同时，LRTA\* **内存占用低**，每次只考虑局部邻域，不需要存储整个大地图的搜索树，因而可应用于超大状态空间（如经典的24拼图问题）。此外，算法通过反复试探和更新启发值，可以在多次执行过程中逐步提高路径质量（学习特性），对重复性的任务具有长远利益。

**缺点**：由于LRTA\* **每次只规划局部最优的一步**，缺乏全局视野，因此常会走弯路，导致总路径相比全局最优路径**偏长**。特别是在启发函数不够精确时，智能体可能陷入所谓“启发式低谷”（heuristic depression）的区域，不断在次优区域来回探索，表现为走很多冤枉路才能逃离局部陷阱。这种现象会增加执行开销，使收敛变慢。尽管 LRTA\* 最终能学习到更好的启发值，但在单次任务中**解往往是次优的**。另外，LRTA\* 假设在执行过程中环境静态不变（启发式学习的是静态地图的距离），如果环境发生变化，算法没有内置的适应机制，需要额外处理。总的来说，LRTA\* 用时间换取了空间和响应速度，其解质量相对于离线A\*有所牺牲。

**应用场景**：LRTA\* 广泛用于**需要实时决策的路径规划**。典型的是**游戏AI**，例如在大型游戏地图中，角色必须即时决定下一步走向而不能停下来长时间思考，此时 LRTA\* 能保证角色平滑移动。在机器人领域，LRTA\* 可用于自主智能体在未知环境中探索：机器人每走一步更新对环境的认知并调整启发，实时前进。尽管路径不一定最短，但这种方法保证了机器人**不停顿地运动**。LRTA\* 也为研究实时决策与强化学习的交叉提供了基础，因为它与某些强化学习方法类似，都在交替规划与执行中逐渐改进策略。总而言之，在**时间约束极严、需在线规划**的情形下（如实时仿真、游戏、中大型未知空间探索），LRTA\* 提供了一种始终让智能体动起来的解法。

### RTAA\*（Real-Time Adaptive A\*）

**基本思路**：**RTAA**\* 算法是对 LRTA\* 的改进，由 Koenig 等人在 2006 年提出。RTAA\* 仍然属于实时启发式搜索范式，即交替规划和执行，但它引入了**自适应的局部搜索深度**和更彻底的启发式更新策略，从而在每一步决策时取得更优的局部方案。具体来说，RTAA\* 在每次行动前执行一个有限深度的 A\* 局部搜索（而非仅一步），搜索深度可以根据可用时间灵活调整（因此称“contract anytime”方法）。完成局部搜索后，**RTAA**\* 会将该局部搜索空间内所有经过的节点的启发值统统更新得更接近真实距离，以便下次搜索时更加高效。通过这种“向前看多步+广泛启发更新”，RTAA\* 相比 LRTA\* 能**避免智能体陷入启发式低谷**，从而规划出代价更低的路径。

**优点**：RTAA\* 最明显的优势是**显著降低了实时搜索下的路径代价**。实验表明，在每步决策耗时相同的条件下，RTAA\* 找到的路径总成本比传统实时启发式方法（如 LRTA\* 或其改进版 LSS-LRTA\*）要低，也就是走得更直、更接近最优。这是因为 RTAA\* **更充分地利用了每一步的规划时间**，通过更深的局部搜索看得更远，且大范围地提升了启发估计的准确性。其次，文献指出 RTAA\* 的实现**相对简单**——它在架构上类似于多次受限A*搜索，主要增加了启发更新步骤，不需要复杂的数据结构维护，这使得工程上更易于采用。再次，RTAA* 仍然保持了实时算法满足硬实时约束的特性：局部搜索空间大小可控，不随全局问题规模增长。因此 RTAA\* 在大型状态空间下也能工作并保证响应速度。

**缺点**：RTAA\* 作为实时算法，依然**无法保证全局最优**。它所提供的是比LRTA*更好的近似解，但受限于每次只能规划有限深度，它的路径可能比离线A*更长。若将搜索深度设得过小，RTAA\* 退化回LRTA*的行为；而深度设大虽能提高解质量，但可能触及实时响应上限，需要在质量和响应之间权衡。此外，在极端复杂的地图或启发不准确的情况下，RTAA* 可能仍会遇到效率瓶颈，需要结合其它策略（例如后来研究者提出的 aRTAA\* 和 daRTAA\* 算法来进一步避免启发低谷）。最后，RTAA\* 假定环境静态，在动态环境中需要和增量算法结合使用。因此，RTAA\* 是专为**静态环境下的实时路径规划**设计的，对动态变化不敏感。

**应用场景**：RTAA\* 适用于**实时性要求最高且希望路径质量较好的场景**。这包括**游戏AI**中复杂地图上的单位导航：相比 LRTA\*，使用 RTAA\* 的 NPC 会走更聪明的路线，同时仍能保证每帧及时决策，不滞后。在机器人学中，RTAA\* 可用于**未知环境**下目标固定的导航任务：机器人每次向目标推进时都做较深的局部搜索，因而比 LRTA\* 少走弯路。RTAA\* 也常被用作实时搜索算法性能的基准，在学术研究中与其它实时规划策略比较。在需要**长期在线运行且追求更优路径**的情境（如持续运行的机器人巡逻、大型多人游戏服务器中的路径服务），RTAA\* 提供了比 LRTA\* 更高效的实时规划方案。

## 算法比较总结表

&#x20;*各算法特性对比表：列出了 Repairing A*（动态A\*）、ARA\*、LPA\*、LRTA\*、RTAA\* 的提出时间、主要特点、优缺点及典型应用场景。\*

| 算法                                   | 提出时间                    | 主要特点                               | 优点                                                                   | 缺点                                                                         | 典型应用场景                                                      |
| ------------------------------------ | ----------------------- | ---------------------------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------------- | ----------------------------------------------------------- |
| **Repairing A**\*<br>*(动态 A*, 如 D\*) | 1994–1995年<br>(Stentz)  | 增量式路径重规划算法；在图发生变化时**局部修复**先前解      | - 能适应**动态环境**，无需每次从头规划<br>- 重用已有搜索信息，提高处理**未知/变化地图**的效率              | - 实现复杂（需维护搜索树状态）<br>- 变化过大时性能退化接近重规划；初始规划仍昂贵                               | 机器人导航（未知或动态环境，如火星探测车）<br>动态路径规划（地图障碍变化）                     |
| **ARA**\*<br>*Anytime Repairing A*   | 2003年<br>(Likhachev等)   | 任意时间算法；先快得次优解再**渐进优化**至最优          | - **快速出解**：用放大启发快速找到可行路径<br>- 解逐步优化，提供次优解误差界<br>- 重用前次搜索结果，较重复规划提速数倍 | - 限静态环境，**不适用于动态变化**场景<br>- 实现较复杂，需维护多次迭代的OPEN表状态<br>- 初始启发权重需慎选，权衡初解速度和质量 | 时间紧迫的路径规划（需快速方案并改进）<br>机器人运动规划（先动起来再优化路径）<br>任何要求渐进改进解的AI求解 |
| **LPA**\*<br>*Lifelong Planning A*   | 2001/2004年<br>(Koenig等) | 增量A\*算法；**重复规划**相似路径问题时重用搜索树       | - 针对小幅图变化高效，重复规划较快<br>- 保证每次路径**全局最优**<br>- 通用框架，被 D\* Lite 等沿用和简化   | - 大幅变化时需更新很多节点，优势减弱<br>- 维护OPEN表有内存开销<br>- 实现复杂度高于标准A\*                    | 持续运行的导航（多次路径规划）<br>动态环境路径规划（变化较平缓的情况）<br>游戏中多次往返寻路          |
| **LRTA**\*<br>*Learning Real-Time A* | 1990年<br>(Korf)         | 实时启发式搜索；**边搜索边执行**，在线更新启发值         | - 满足**硬实时**要求，每步规划开销小<br>- 内存占用低，适用超大状态空间<br>- 可在多次任务中学习启发值，逐步改进性能   | - 路径次优：只看局部易走弯路<br>- 可能陷入启发局部低估区，来回探索<br>- 假定环境静止不变，无法直接应对变化               | 游戏AI实时导航（如即时战略单位移动）<br>未知环境探索（机器人每步探路前进）<br>需要持续动作响应的规划任务   |
| **RTAA**\*<br>*Real-Time Adaptive A* | 2006年<br>(Koenig等)      | 改进的实时搜索；每步执行前做**有限深度A**\*搜索并全局更新启发 | - 路径质量较LRTA\*显著提升<br>- 充分利用每步时间，看得更远不易陷入死胡同<br>- 保持实时响应；实现相对简单       | - 仍非全局最优，受限于局部搜深<br>- 深度参数需权衡，否则过浅退化为LRTA\*<br>- 假定环境静态，动态变化需另辅以增量算法       | 游戏AI复杂路径实时规划<br>长期运行的实时导航（追求较优路径）<br>作为实时算法基准衡量其它方法         |

## 技术异同分析

上述五种算法都源自经典 A\* 算法的思想，但针对不同需求对其进行了扩展和改进。从对比可以看出，它们在**优化目标和应用场景**上各有侧重，可大致分为三类：

* **增量式路径规划算法（Repairing A* 和 LPA*）**：这一类算法关注**环境变化或重复规划**问题。它们假设搜索空间可能发生变化或者需要多次求解类似问题，通过重用先前计算结果来加速后续的规划。Repairing A\*（以动态A*为代表）和 LPA* 均属于增量搜索：前者侧重机器人实时避障等动态环境，后者提供了通用框架用于重复路径求解。二者的相似点是都**保持并更新了搜索树的状态\*\*，在需要重新规划时不从零开始；差异在于 Repairing A\* 更强调**执行时环境变化**（如障碍出现/消失），而 LPA\* 通常假定一系列规划问题之间差异较小，属于“生命周期”内多次查询。相应地，Repairing A\*（D\*）在机器人导航中应用更广，而 LPA\* 作為学术框架被后续算法（如 D\* Lite）采用改进。这两类算法在环境变化较小时效果最佳，当变化剧烈时都会退化成重新规划。

* **任意时间算法（ARA*）\*\*：ARA* 所代表的是**时间和解质量权衡**的路径规划方法。它不针对环境变化，而是假设问题静态不变，但时间有限。ARA\* 的独特之处在于提出了解决“**及时提供可用解**”的问题：在必须很快给出路径的情况下，允许解不最优，然后利用剩余时间逐步逼近最优。相比增量算法和实时算法，ARA\* 更像是经典离线搜索的扩展，即算法本身仍然是离线运行，只不过可以随时中断并得到当前最好的解。它和增量算法的区别在于：增量算法针对**输入的变化**，而ARA*针对**计算时间的分配**。在应用上，ARA* 适用于静态规划场景但对计算时间要求苛刻的情况，例如机器人在复杂环境中启程前需要尽快一条路径，然后在行进过程中继续优化（假设环境不变）。需要注意的是，ARA\* 可以与增量思想结合以应对动态环境（如 ADA\*），但单纯ARA\*并未设计用于环境会变化的过程。

* **实时启发式搜索（LRTA* 和 RTAA*）**：这两种算法面向**在线决策\*\*，要求算法在行动同时规划。LRTA\* 是早期方案，每次只规划一步并即时执行，算法非常简单（类似贪心走一步+启发学习）。RTAA\* 则在LRTA*基础上进行了改进，在每个动作决策阶段进行更深入的局部搜索并大范围更新启发。两者均保证了动作的**持续性**和**实时性**，适合游戏和实时仿真环境。其中 RTAA* 通常能获得比 LRTA\* **更短的路径**，因为它克服了很多 LRTA\* 的不足，如减少了在启发不准确区域来回探索的情况。可以认为，RTAA\* 提供了在给定每步时间限制下更\*\*“充分利用时间”**的策略，而 LRTA* 是*\*“用最小时间前进一小步”**的策略。尽管如此，和任意时间ARA\*算法相比，这两种实时算法追求的不是解逐渐达最优，而是**始终满足实时响应\*\*，因而容忍一定的次优解代价。这也是实时搜索和离线/任意时间搜索的根本区别：前者保证响应速度，后者追求最终最优。

总的来说，这些算法各自针对**不同的问题维度**进行了优化：有的侧重**环境动态适应**(Repairing A\*, LPA\*)，有的侧重**计算时间利用**(ARA\*)，有的侧重**在线实时决策**(LRTA\*, RTAA\*)。选择何种算法应根据应用需求：如果地图会变化，应选用增量/修复型算法；如果需要随时可中断并逐步改进解，任意时间算法ARA*是理想选择；如果要求持续实时行动，LRTA*/RTAA*不可或缺。在机器人自主导航中常将增量算法与实时算法结合，例如机器人每步用实时算法前进，同时环境变化时触发增量重规划。在游戏AI中，由于环境通常静态且需要实时反应，可优先采用 LRTA* 或 RTAA\*。而在路径规划比赛或要求高质量解但有限时间的场合，ARA\*提供了折中方案。

通过上述比较可以看出，**没有“一刀切”的最优算法**，每种算法都有其**适用领域**和**局限性**。研究者在 A\* 框架下发展出这些变体，正是为了在不同场景下权衡最优性、速度、动态性等因素。今后，随着计算资源和应用需求的变化，还会有新的改进算法出现，例如结合机器学习预测启发的路径规划、更加智能的实时搜索等。但理解这些经典算法的异同，有助于我们根据具体问题选择合适的路径规划方法，并为改进现有算法提供思路。各算法的对比如上表所示，我们应根据问题特性权衡采用，以在**最短时间**内找到**足够好的路径**，并在需要时进一步优化或适应新的环境。&#x20;
